{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f80785",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "Module 11 Project: Image recognition of letters **A**, **B**, and **C** using a 2-layer neural network implemented with NumPy only.\n",
    "\n",
    "This notebook defines binary 5x6 pixel patterns for each letter, builds and trains a small feedforward neural network with one hidden layer using sigmoid activation, and evaluates it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4502a7ef",
   "metadata": {},
   "source": [
    "## 1. Create binary 5x6 patterns for A, B, C\n",
    "\n",
    "We define each letter as a 5x6 (width x height = 5 x 6 -> 30 pixels) binary pattern and flatten to 30-element vectors. We'll create a small dataset by adding slight noise variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb70bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "A = np.array([\n",
    "    [0,1,1,1,0],\n",
    "    [1,0,0,0,1],\n",
    "    [1,1,1,1,1],\n",
    "    [1,0,0,0,1],\n",
    "    [1,0,0,0,1],\n",
    "    [1,0,0,0,1]\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [1,1,1,1,0],\n",
    "    [1,0,0,0,1],\n",
    "    [1,1,1,1,0],\n",
    "    [1,0,0,0,1],\n",
    "    [1,0,0,0,1],\n",
    "    [1,1,1,1,0]\n",
    "])\n",
    "\n",
    "C = np.array([\n",
    "    [0,1,1,1,1],\n",
    "    [1,0,0,0,0],\n",
    "    [1,0,0,0,0],\n",
    "    [1,0,0,0,0],\n",
    "    [1,0,0,0,0],\n",
    "    [0,1,1,1,1]\n",
    "])\n",
    "\n",
    "def flatten(letter):\n",
    "    return letter.reshape(-1)\n",
    "\n",
    "def noisy_versions(base, n=50, noise_level=0.05):\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        mat = base.copy().astype(float)\n",
    "        flip = np.random.rand(*mat.shape) < noise_level\n",
    "        mat = np.where(flip, 1-mat, mat)\n",
    "        samples.append(flatten(mat))\n",
    "    return np.array(samples)\n",
    "\n",
    "n_per = 100\n",
    "data_A = noisy_versions(A, n=n_per, noise_level=0.05)\n",
    "data_B = noisy_versions(B, n=n_per, noise_level=0.05)\n",
    "data_C = noisy_versions(C, n=n_per, noise_level=0.05)\n",
    "\n",
    "X = np.vstack([data_A, data_B, data_C])\n",
    "y_labels = np.array([0]*n_per + [1]*n_per + [2]*n_per)\n",
    "\n",
    "Y = np.zeros((y_labels.size, 3))\n",
    "Y[np.arange(y_labels.size), y_labels] = 1\n",
    "\n",
    "print('Dataset shape X:', X.shape, 'Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20667456",
   "metadata": {},
   "source": [
    "## 2. Build a 2-layer Neural Network (NumPy only)\n",
    "\n",
    "Architecture: Input (30) -> Hidden (16) -> Output (3). Activation: sigmoid. Loss: Mean Squared Error (for simplicity).\n",
    "Training with batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "input_size = 30\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    "rng = np.random.default_rng(1)\n",
    "W1 = rng.normal(0, 0.5, (input_size, hidden_size))\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = rng.normal(0, 0.5, (hidden_size, output_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "lr = 0.8\n",
    "epochs = 1000\n",
    "\n",
    "perm = np.random.permutation(X.shape[0])\n",
    "X_shuffled = X[perm]\n",
    "Y_shuffled = Y[perm]\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1 = X_shuffled.dot(W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = A1.dot(W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    loss = mse_loss(Y_shuffled, A2)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    preds = np.argmax(A2, axis=1)\n",
    "    true = np.argmax(Y_shuffled, axis=1)\n",
    "    acc = np.mean(preds == true)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    dA2 = -(Y_shuffled - A2) * (2 / Y_shuffled.size)\n",
    "    dZ2 = dA2 * sigmoid_derivative(A2)\n",
    "    dW2 = A1.T.dot(dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    dA1 = dZ2.dot(W2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)\n",
    "    dW1 = X_shuffled.T.dot(dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    \n",
    "    if (epoch+1) % 200 == 0 or epoch==0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.5f} - Acc: {acc:.3f}\")\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d05b2",
   "metadata": {},
   "source": [
    "## 3. Training curves (Loss & Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_history)\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7ffe2",
   "metadata": {},
   "source": [
    "## 4. Evaluate on held-out noisy examples and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42260bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test examples (clean + noisy)\n",
    "test_clean = np.vstack([flatten(A), flatten(B), flatten(C)])\n",
    "test_noisy = np.vstack([noisy_versions(A, n=5, noise_level=0.08),\n",
    "                        noisy_versions(B, n=5, noise_level=0.08),\n",
    "                        noisy_versions(C, n=5, noise_level=0.08)])\n",
    "X_test = np.vstack([test_clean, test_noisy])\n",
    "y_test = np.array([0,1,2] + [0]*5 + [1]*5 + [2]*5)\n",
    "Y_test = np.zeros((y_test.size, 3)); Y_test[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "A1_test = sigmoid(X_test.dot(W1) + b1)\n",
    "A2_test = sigmoid(A1_test.dot(W2) + b2)\n",
    "preds = np.argmax(A2_test, axis=1)\n",
    "acc_test = np.mean(preds == y_test)\n",
    "print('Test accuracy:', acc_test)\n",
    "\n",
    "labels = ['A','B','C']\n",
    "import matplotlib.pyplot as plt\n",
    "n_show = X_test.shape[0]\n",
    "cols = 5\n",
    "rows = int(np.ceil(n_show/cols))\n",
    "plt.figure(figsize=(12, 3*rows))\n",
    "for i in range(n_show):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(X_test[i].reshape(6,5), cmap='gray_r', interpolation='nearest')\n",
    "    plt.title(f\"True: {labels[y_test[i]]} - Pred: {labels[preds[i]]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
